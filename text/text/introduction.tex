\chapter{Einführung}

% \begin{itemize}
%     \item Finden von Lösungen ohne große Anpassung von Hyperparamteren
%     \item Domänen mit starken Einschränkungen (sparse Fitness, hochdimensionale kontinuierlicher Zustandsraum)
%     \item Maschinelles Lernen ist eine potenziell große Hilfe zur Entwicklung von besseren Systemen
% \end{itemize}

Die Relevanz von Machine Learning Algorithmen und \textbf{Deep Learning} \cite{dloverview} hat in den letzten Jahren seit der Weiterentwicklung von \textbf{GPUs} (Graphics Processing Unit) stark zugenommen. Das Training wird dabei durch die Optimierungsmethode \textbf{SGD} (Stochastic Gradient Descend) durchgeführt, die uns erlaubt durch das Ableiten einer multidimensionalen Funktion zu einer Lösung zu konvergieren. Damit wurden bemerkenswerte Maßstäbe in der Beschreibung von Bildern in \textbf{ImageNet} \cite{NIPS2012_4824}, dem Lernen einer Strategie für das Brettspiel \textbf{Go} \cite{go} oder der Nachahmung der menschlichen Sprache durch \textbf{WaveNet} \cite{wavenet} gesetzt. \\

\noindent
Leider sind dadurch andere Methoden zur Entwicklung von neuronalen Netzen aus dem Fokus gefallen, die zu der Familie von \textbf{unsupervised Learning} gehören. Sie können vielseitiger eingesetzt werden, weil sie weniger Einschränkungen für die Anwendungsdomäne haben. Sie benötigen keine vorher beschriftete Daten und eignen sich für simulationbasiertes Training mit seltenen Fitnesssignalen.\\

\noindent
Insbesondere untersuchen wir den \textbf{CoSyNE} Algorithmus, der verschiedene Techniken verknüpft, um die Suche im Raum von neuronalen Netzen zu beschleunigen. Dabei zeigen wir einen bisher nicht gesehenen Vergleich mit dem \textbf{Neuroevolutionsalgorithmus} ohne den zusätzlichen Permutationsschritt. Hinzu kommt der \textbf{Kompressionsfaktor von 1:55} im Gewichtsraum für ein größeres rekurrenten Netz als in der Literatur \cite{cosyne1}.

\section{Aufgabenstellung}

In dieser Arbeit beschäftigen wir uns mit der Entwicklung von neuronalen Netzen mithilfe von genetischen Algorithmen für die Fußballdomäne \textbf{Half Field Offense} \cite{hfo}. Sie hat ein \textbf{spärliches Fitnesssignal} ohne Gradient, ein hochdimensionalen kontinuierlichen Zustandsraum und keine Möglichkeit für jede Situation eine perfekte Aktion festzulegen. Damit bietet sie Parallelen zu echte-welt Problemen für die man entweder nicht genug Wissen sammeln konnte, oder wollte.  \\

\noindent
Wir untersuchen verschiedene Kodierungen und Implementierungen für neuroevolutionäre Algorithmen und versuchen den Nutzen für andere Domänen mit ähnlichen Einschränkungen zu erahnen.\\


\newpage
\section{Motivation}

Die Industrie interessiert sich für allgemeine Problemlösungen, die in kurzer Zeit, mit wenig Daten und am besten automatisch zu einem akzeptablen Ergebniss kommt. Leider steht das den üblichen \textbf{Deep Learning} Techniken gegenüber, die lange Trainigszeiten haben, viele nicht homogene Daten in normalisierter Form brauchen und Fitness Funktionen benötigen, die von Hand für das Ziel angepasst wurden. \\

\noindent
Deshalb betrachten wir verschiedene Möglichkeiten mithilfe eines genetischen Algorithmus neuronalen Netze zu entwickeln, die als Fitnessignal lediglich das Ziel bekommen und sich in einem hochdimensionalen, stetig verändernden, kontinuierlichen Zustandsraum mit mehreren Akteuren bewegen.

\section{Aufbau der Arbeit}

Im Rahmen dieser Arbeit erklären wir im Kapitel 2 die Grundlagen von genetischen Algorithmen, die Hinführung zur Entwicklung von neuronalen Netzen, den CoSyNE Algorithmus und die Cross Entropy Method. Kapitel 3 beschäftigt sich mit der Domäne und der Parametrisierung aller getesteten Algorithmen. Im Kapitel 4 beschreiben wir die Resultate und vergleichen sie untereinander. Das Kapitel 5 gibt einen Ausblick in weitere Verbesserungsmöglichkeiten, stellt Vergleiche zu bisherigen Resultaten von CoSyNE dar und behandelt verwandte Felder.